决策树既可以作为分类算法，也可以作为回归算法，同时有可以用于集成学习如随机森林
决策树思想：if,else...
思考：有许多条件，用哪个条件特征先做if，哪个条件特征后做if才能使得结果更优呢？
      所以关键点在于如何选择标准
信息论中的熵---衡量事物的不确定性，越不确定的事物，熵越大。
1.决策树的ID3算法---用信息增益来判断当前节点该用什么特征来构建决策树，信息增益越大，则越适合用来分类。
2.决策树的C4.5算法---在ID3的基础上可以处理连续特征了（用信息增益比来选择节点），可以处理缺失值问题，用的是星系增益比来选择特征（可以减少信息增益易选择特征值多的特征问题）。
ID3算法和C4.5算法都只能处理分类问题，无法处理回归问题。
3.CART算法---更加进一步的改进，在不考虑集成学习的前提下，该算法为较优算法，用的是基尼系数---表示模型的不纯度，基尼系数越小，则特征越优。
  可做回归，可做分类。
  CART算法只对特征值进行二分，可以理解为二叉树，简化计算。
  对于连续问题，将其离散化，用基尼系数来选择节点，基尼系数最小点为节点----分类数。
                          ，用和方差的度量方式，均方差和最小的所对应的特征值为节点---回归树。
  与C4.5不同的是，在CART中离散特征可以参与多次节点的建立。
  思路：不停的进行二分离散特征，对各种方案进行对比，选择最小的基尼系数组合。
    CART分类数和CART回归树的不同：样本输出的不同，分类数---离散值，连续值---回归树。连续值的处理方法不同。决策树建立后的预测方式不同。
决策树算法容易过拟合，所以需要“剪枝”（类比于线性回归的正则化）
如何选择剪枝方法？
CART采用先生成决策树，后剪枝，用交叉验证来检验各种剪枝的效果，选取泛化能力最好的剪枝策略。


